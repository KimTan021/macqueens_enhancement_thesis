from llama_index.core import Settings
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.groq import Groq
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import streamlit as st
from streamlit_chat import message


@st.cache_resource
def load_model():
    """
    Loads the model and initializes the necessary components for the chatbot.

    Returns:
        query_engine (QueryEngine): The initialized query engine for the chatbot.
    """
    llm = Groq(model="llama3-70b-8192", api_key=st.secrets["API_KEY"])
    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
    Settings.llm = llm
    Settings.embed_model = embed_model
    documents = SimpleDirectoryReader(input_files=['data/TAN_TIANGCO_MACQUEEN_FINAL_FINAL_FINAL.pdf'], ).load_data()
    index = VectorStoreIndex.from_documents(documents)
    query_engine = index.as_query_engine()

    return query_engine

@st.cache_data
def get_response_from_chatbot(user_query):
    """
    Get the response from the chatbot model for a given user query.

    Parameters:
    user_query (str): The query entered by the user.

    Returns:
    tuple: A tuple containing the response and source nodes.
           - response (str): The response generated by the chatbot model.
           - source_nodes (list): The list of source nodes used by the model to generate the response.

    Raises:
    ValueError: If the model loading fails.

    """
    try:
        query_engine = load_model()
        if query_engine is None:
            raise ValueError("Model loading failed")

        # Get response from the model
        response = query_engine.query(user_query)
    except Exception as e:
        st.error(f"An error occurred: {e}")
        return

    return response.response, response.source_nodes


def app():
    st.write("Our study, titled 'Enhanced MacQueen's Algorithm for Crime Pattern Analysis,' enables this "
             "Crime Statistics web app. To ensure accurate crime statistics, we enhance MacQueen's clustering "
             "algorithm to improve its performance.")
    st.write("### ðŸ‘‹Hi! Do you have any questions about our study?")

    # Input text box for user query
    user_query = st.text_input("Ask a question:")

    if user_query:
        with st.spinner('Generating response...'):
            response, source_nodes = get_response_from_chatbot(user_query)
        # Display the user query and response in chat format
        message(user_query, is_user=True)
        message(response, is_user=False)

        # st.markdown("### References:")
        # for node in source_nodes:
        #     message(node.text, is_user=False)